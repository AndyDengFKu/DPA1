{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMkQZRE3Z3K37d75JdISV2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyDengFKu/DPA1/blob/main/DPL_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MfEQ-R1_mklG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab433b57-756f-4689-96d7-b6ee9f5d13d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from LIBSVM format\n",
        "def load_libsvm_format(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    max_feature_index = 0\n",
        "\n",
        "    for line in lines:\n",
        "        items = line.strip().split()\n",
        "        labels.append(int(items[0]))\n",
        "\n",
        "        features = {}\n",
        "        for item in items[1:]:\n",
        "            index, value = item.split(\":\")\n",
        "            index = int(index)\n",
        "            features[index] = float(value)\n",
        "            if index > max_feature_index:\n",
        "                max_feature_index = index\n",
        "\n",
        "        data.append(features)\n",
        "\n",
        "    # Convert to matrix format\n",
        "    matrix_data = np.zeros((len(data), max_feature_index))\n",
        "    for i, row in enumerate(data):\n",
        "        for index, value in row.items():\n",
        "            matrix_data[i][index - 1] = value\n",
        "\n",
        "    return np.array(matrix_data), np.array(labels)\n",
        "\n",
        "# Ensure that X_train and X_val have the same number of features\n",
        "def align_features(X_train, X_val):\n",
        "    num_features_train = X_train.shape[1]\n",
        "    num_features_val = X_val.shape[1]\n",
        "\n",
        "    if num_features_train > num_features_val:\n",
        "        # Add missing columns to X_val\n",
        "        missing_cols = np.zeros((X_val.shape[0], num_features_train - num_features_val))\n",
        "        X_val = np.hstack((X_val, missing_cols))\n",
        "\n",
        "    elif num_features_train < num_features_val:\n",
        "        # Add missing columns to X_train\n",
        "        missing_cols = np.zeros((X_train.shape[0], num_features_val - num_features_train))\n",
        "        X_train = np.hstack((X_train, missing_cols))\n",
        "\n",
        "    return X_train, X_val\n",
        "\n",
        "\n",
        "# Load training data\n",
        "X_train, y_train = load_libsvm_format(\"/content/drive/MyDrive/Colab Notebooks/DeepLearning/A1/a1.txt\")\n",
        "\n",
        "# Load validation data\n",
        "X_val, y_val = load_libsvm_format(\"/content/drive/MyDrive/Colab Notebooks/DeepLearning/A1/a1a.txt\")\n",
        "\n",
        "# Align the features of training and validation data\n",
        "X_train_aligned, X_val_aligned = align_features(X_train, X_val)\n",
        "\n",
        "print(f\"Training data shape: {X_train_aligned.shape}\")\n",
        "print(f\"Validation data shape: {X_val_aligned.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2KYvlUovAD",
        "outputId": "db396e75-5df5-4954-af38-94149ca283e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1605, 123)\n",
            "Validation data shape: (30956, 123)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.where(linear_output > 0, 1, -1)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training loop\n",
        "        for _ in range(self.epochs):\n",
        "            for idx, xi in enumerate(X):\n",
        "                update = self.learning_rate * (y[idx] - self.predict(xi))\n",
        "                self.weights += update * xi\n",
        "                self.bias += update\n",
        "\n",
        "# Initialize perceptron model\n",
        "perceptron = Perceptron(learning_rate=0.01, epochs=5000)\n",
        "\n",
        "# Train the model\n",
        "perceptron.train(X_train_aligned, y_train)\n",
        "\n",
        "# Predict on validation data\n",
        "y_pred_val = perceptron.predict(X_val_aligned)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred_val == y_val)\n",
        "\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv8sjqDorq4T",
        "outputId": "cb20cffa-ca8d-4ce4-b9bf-c971e7910a9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8220054270577594"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def cross_validate(model, X, y, n_splits=5):\n",
        "    \"\"\"\n",
        "    Perform cross-validation on the provided model and data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The machine learning model to be evaluated.\n",
        "    - X: Features.\n",
        "    - y: Labels.\n",
        "    - n_splits: Number of splits for cross-validation.\n",
        "\n",
        "    Returns:\n",
        "    - List of accuracy scores for each fold.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "        model.train(X_train_fold, y_train_fold)\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "\n",
        "        accuracy = np.mean(y_pred == y_val_fold)\n",
        "        scores.append(accuracy)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Now performing the cross-validation on the perceptron model\n",
        "cv_scores = cross_validate(Perceptron(learning_rate=0.01, epochs=1000), X_train_aligned, y_train)\n",
        "\n",
        "cv_scores, np.mean(cv_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lUwO_2vk6-8",
        "outputId": "b23d4dab-7fc5-440c-f94c-9ae5271b4bbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.8099688473520249,\n",
              "  0.8255451713395638,\n",
              "  0.8130841121495327,\n",
              "  0.7757009345794392,\n",
              "  0.8348909657320872],\n",
              " 0.8118380062305295)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the original Perceptron model to experiment with different learning rates\n",
        "class SimplePerceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.where(linear_output > 0, 1, -1)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "        for _ in range(self.epochs):\n",
        "            for idx, xi in enumerate(X):\n",
        "                update = self.learning_rate * (y[idx] - self.predict(xi))\n",
        "                self.weights += update * xi\n",
        "                self.bias += update\n",
        "\n",
        "# Experiment with different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5]\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    perceptron = SimplePerceptron(learning_rate=lr, epochs=1000)\n",
        "    perceptron.train(X_train_scaled, y_train)\n",
        "    y_pred_val = perceptron.predict(X_val_scaled)\n",
        "    accuracy = np.mean(y_pred_val == y_val)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "accuracies\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSdkz2-redzZ",
        "outputId": "942874e3-d988-4bd7-9374-d7026be5c299"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7911874919240212,\n",
              " 0.7911874919240212,\n",
              " 0.7911874919240212,\n",
              " 0.7911874919240212,\n",
              " 0.7911874919240212]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize MLP model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, activation='relu', solver='adam', random_state=1)\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation data\n",
        "y_pred_mlp = mlp.predict(X_val_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_mlp = np.mean(y_pred_mlp == y_val)\n",
        "print(\"MLP Accuracy:\", accuracy_mlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65PO-xEGgQGC",
        "outputId": "da818d1a-3ae4-42b8-82dd-ca73f622ab6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Accuracy: 0.8150277813671017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Setting up a parameter grid for grid search\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(100,), (50, 50), (30, 30, 30)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam'],\n",
        "    'learning_rate_init': [0.001, 0.01],\n",
        "    'alpha': [0.0001, 0.001]  # L2 regularization term parameter\n",
        "}\n",
        "\n",
        "# Initializing GridSearch with MLPClassifier and the parameter grid\n",
        "grid_search = GridSearchCV(MLPClassifier(max_iter=1000, random_state=1), param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Getting the best parameters and the associated accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuqcMgLth_O0",
        "outputId": "f4d0ae7b-bd51-4deb-c822-ff32739820e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "Best Parameters: {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
            "Best Score: 0.8280373831775701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize MLP model with the best parameters\n",
        "mlp_best = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    activation='logistic',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.01,\n",
        "    alpha=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Train the model with the best parameters on the entire training set\n",
        "mlp_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation data\n",
        "y_pred_mlp_best = mlp_best.predict(X_val_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_mlp_best = np.mean(y_pred_mlp_best == y_val)\n",
        "\n",
        "print(\"MLP (Best Parameters) Accuracy on Validation Set:\", accuracy_mlp_best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIyI5kcBlGHz",
        "outputId": "d72ae6f5-6493-4567-dfbb-73b8f4985214"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP (Best Parameters) Accuracy on Validation Set: 0.8219408192272903\n"
          ]
        }
      ]
    }
  ]
}